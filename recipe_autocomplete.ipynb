{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:33:38.586582Z",
     "start_time": "2024-05-31T02:33:33.732755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk import SnowballStemmer, sent_tokenize,word_tokenize\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "        # Tokenize by sentence, then by word\n",
    "\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "\n",
    "        # Filter out raw tokens to remove noise\n",
    "        filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "        # print('filtered tokens ')\n",
    "        # print(filtered_tokens)\n",
    "\n",
    "        # Stem the filtered_tokens\n",
    "        stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "        return stems\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def setup( *args):\n",
    "        for arg in args:\n",
    "            if 'query' not in db:\n",
    "                db['query'] = ''\n",
    "            db['query'] += (db[arg].astype(str)+'\\n')\n",
    "            \n",
    "        # print(db['query'].isnull().sum())  # Check if there are NaN values\n",
    "        # print((db['query'] == '').sum()) \n",
    "        # print(self.db['query'])\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                           min_df=0.1, stop_words='english',\n",
    "                                    tokenizer=tokenize_and_stem,\n",
    "                                           ngram_range=(1, 3))\n",
    "\n",
    "        # db.drop_duplicates(subset='name', inplace=True)\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in db['query']])\n",
    "        print(tfidf_matrix)\n",
    "        similarity_distance = 1 - cosine_similarity(tfidf_matrix)\n",
    "        print(cosine_similarity(tfidf_matrix))\n",
    "        # cosine_sim_df = pd.DataFrame(similarity_distance, index=db['name'], columns=db['name'])\n",
    "        # print(db['name'].duplicated().sum())\n",
    "\n",
    "\n",
    "        return similarity_distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def find_ksimilar(title: str, k, titleCol,similarity_distance):\n",
    "        index = db[db[titleCol].str.lower() == title.lower()]\n",
    "        # print(self.db[self.db[titleCol].str.lower() == title.lower()])\n",
    "        # print(index)\n",
    "        index = db[db[titleCol].str.lower() == title.lower()].index[0]\n",
    "        # print(index)\n",
    "        vector = similarity_distance[index]\n",
    "    \n",
    "        print(vector)\n",
    "        most_similar = db.iloc[np.argsort(vector)[1:k + 1], 1]\n",
    "        return most_similar\n",
    "\n",
    "db = pd.read_csv('./datasets/automation_recipes1.csv')\n",
    "dbPath = './datasets/automation_recipes1.csv'\n",
    "similarity_dist = setup('name')\n",
    "print(find_ksimilar('Noodles with Tofu', 10,'name',similarity_dist))"
   ],
   "id": "f02322a6be0238e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seymour-butts/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/seymour-butts/miniconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (34, 0)\t1.0\n",
      "  (40, 0)\t1.0\n",
      "  (52, 0)\t1.0\n",
      "  (53, 0)\t1.0\n",
      "  (60, 0)\t1.0\n",
      "  (104, 0)\t1.0\n",
      "  (108, 0)\t1.0\n",
      "  (109, 0)\t1.0\n",
      "  (112, 0)\t1.0\n",
      "  (122, 0)\t1.0\n",
      "  (133, 0)\t1.0\n",
      "  (142, 0)\t1.0\n",
      "  (162, 0)\t1.0\n",
      "  (163, 0)\t1.0\n",
      "  (170, 0)\t1.0\n",
      "  (171, 0)\t1.0\n",
      "  (173, 0)\t1.0\n",
      "  (192, 0)\t1.0\n",
      "  (193, 0)\t1.0\n",
      "  (195, 0)\t1.0\n",
      "  :\t:\n",
      "  (30532, 0)\t1.0\n",
      "  (30561, 0)\t1.0\n",
      "  (30563, 0)\t1.0\n",
      "  (30565, 0)\t1.0\n",
      "  (30570, 0)\t1.0\n",
      "  (30621, 0)\t1.0\n",
      "  (30623, 0)\t1.0\n",
      "  (30625, 0)\t1.0\n",
      "  (30626, 0)\t1.0\n",
      "  (30627, 0)\t1.0\n",
      "  (31348, 0)\t1.0\n",
      "  (31349, 0)\t1.0\n",
      "  (31350, 0)\t1.0\n",
      "  (32189, 0)\t1.0\n",
      "  (32534, 0)\t1.0\n",
      "  (32560, 0)\t1.0\n",
      "  (32682, 0)\t1.0\n",
      "  (34385, 0)\t1.0\n",
      "  (34663, 0)\t1.0\n",
      "  (34699, 0)\t1.0\n",
      "  (34712, 0)\t1.0\n",
      "  (34718, 0)\t1.0\n",
      "  (34727, 0)\t1.0\n",
      "  (34739, 0)\t1.0\n",
      "  (34800, 0)\t1.0\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "23260                                Pirate Ship Pork Loin\n",
      "23259                                         Pirate Pasta\n",
      "23258    Piquillo Peppers Stuffed with Spicy Salmon Tar...\n",
      "23257                                    Piquillo Frittata\n",
      "23256                                   Piperade Basquaise\n",
      "23255                    Pioneer's Scone and Lamb Sandwich\n",
      "23254                                     Pinwheels 3 Ways\n",
      "23253                                      Pinwheel Steaks\n",
      "23252                                     Pintor's Chicken\n",
      "23251                    Pinto Bean Soup with Salsa Fresca\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:33:38.589621Z",
     "start_time": "2024-05-31T02:33:38.588138Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "95efb395bdaec88f",
   "outputs": [],
   "execution_count": 64
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
